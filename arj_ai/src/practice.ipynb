{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "upset-collection",
   "metadata": {},
   "source": [
    "# Mesterséges Intelligencia - Gyakorlat\n",
    "\n",
    "### 2D Obejktumdetektálás YOLO-val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-portfolio",
   "metadata": {},
   "source": [
    "# 1. YOLOv3\n",
    "\n",
    "![01_yolov1](./doc/01_yolov1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-edward",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![04_yolov3](./doc/04_yolov3.png)\n",
    "\n",
    "### Anchor boxes:\n",
    "\n",
    "![05_yolov3](./doc/05_yolov3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import *\n",
    "from utils.datasets import *\n",
    "from utils.transforms import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-spring",
   "metadata": {},
   "source": [
    "# 2. Dataset and labels\n",
    "\n",
    "## KITTI\n",
    "\n",
    "Training dataset: 7481 labelled images \n",
    "\n",
    "http://www.cvlibs.net/datasets/kitti/\n",
    "![06_kitti.png](./doc/06_kitti.png)\n",
    "\n",
    "![08_kitti.jpg](./doc/08_kitti.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Define kitti data directories\n",
    "data_dir = \"doc/kitti/\"\n",
    "img_dir = os.path.join(data_dir, \"images\")\n",
    "label_dir = os.path.join(data_dir, \"labels\")\n",
    "\n",
    "# Initialize plot\n",
    "fig, axs = plt.subplots(len(os.listdir(img_dir)), figsize=(20, 35))\n",
    "\n",
    "# Plot images and labels\n",
    "for idx, img_name in enumerate(os.listdir(img_dir)):\n",
    "    # Read image from disk\n",
    "    im = Image.open(os.path.join(img_dir, img_name))\n",
    "    axs[idx].imshow(im)\n",
    "\n",
    "    # Read labels from disk and plot 2D bounding box with class name\n",
    "    with open(os.path.join(label_dir, img_name.split(\".\")[0] + \".txt\")) as label_file:\n",
    "        labels = label_file.readlines()\n",
    "        for label in labels:\n",
    "            label_data = label.split(\" \")\n",
    "\n",
    "            # Extract class (cls) and box top left corner (x1, y1) and box dimensions (box_w, box_h)\n",
    "            cls = label_data[0]\n",
    "            x1, y1 = float(label_data[4]), float(label_data[5])\n",
    "            box_w, box_h = (\n",
    "                float(label_data[6]) - float(label_data[4]),\n",
    "                float(label_data[7]) - float(label_data[5]),\n",
    "            )\n",
    "\n",
    "            bbox = patches.Rectangle(\n",
    "                (x1, y1), box_w, box_h, linewidth=2, edgecolor=\"r\", facecolor=\"none\"\n",
    "            )\n",
    "            axs[idx].add_patch(bbox)\n",
    "            axs[idx].text(\n",
    "                x1,\n",
    "                y1,\n",
    "                s=cls,\n",
    "                color=\"white\",\n",
    "                verticalalignment=\"top\",\n",
    "                bbox={\"color\": \"r\", \"pad\": 0},\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    print(\n",
    "        label\n",
    "    )  # type, truncated, occluded, alpha, 2D bbox (x1, y1, x2, y2), dim (h, w, l), 3D loc (x, y, z), rotation_y, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-bailey",
   "metadata": {},
   "source": [
    "Labels for YOLOv3:\n",
    "* Box center (x,y) + Box width/height instead of box edges (x1, y1, x2, y2)\n",
    "* x, y, w, h are relative to image size\n",
    "![07_labels.png](./doc/07_labels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-nigeria",
   "metadata": {},
   "source": [
    "# 3. YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "img_path = \"./data/kitti/images/test/002114.png\"  # feel free to try a different image in this folder!\n",
    "model_def = \"./config/yolov3-kitti-tiny.cfg\"\n",
    "weights_path = \"./weights/yolov3-kitti-tiny.pth\"\n",
    "class_path = \"./data/kitti/classes.names\"\n",
    "conf_thres = 0.8\n",
    "nms_thres = 0.4\n",
    "iou_thres = 0.5\n",
    "n_cpu = 0\n",
    "img_size = 352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-grave",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Load image\n",
    "img = np.array(Image.open(img_path).convert(\"RGB\"), dtype=np.uint8)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Placeholder\n",
    "boxes = np.zeros((1, 5))\n",
    "\n",
    "img, _ = transforms.Compose([DEFAULT_TRANSFORMS, Resize(img_size)])((img, boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model = Darknet(model_def, img_size=img_size).to(device)\n",
    "\n",
    "# Load checkpoint weights\n",
    "model.load_state_dict(torch.load(weights_path, map_location=torch.device(\"cpu\")))\n",
    "\n",
    "# Set model in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class names\n",
    "classes = load_classes(class_path)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure input\n",
    "img = Variable(img.type(torch.FloatTensor))\n",
    "img.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detections\n",
    "with torch.no_grad():\n",
    "    detections = model(img)\n",
    "    print(detections.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "print(detections[0].shape)\n",
    "print(detections[0])  # x1, y1, x2, y2, conf, class1, class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Bounding-box colors\n",
    "cmap = plt.get_cmap(\"tab20b\")\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "# Plot detections\n",
    "# Create plot\n",
    "img = np.array(Image.open(img_path))\n",
    "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
    "ax.imshow(img)\n",
    "\n",
    "# Draw bounding boxes and labels of detections\n",
    "if detections is not None:\n",
    "    # Rescale boxes to original image\n",
    "    detections_rescaled = rescale_boxes(detections[0], img_size, img.shape[:2])\n",
    "    unique_labels = detections_rescaled[:, -1].cpu().unique()\n",
    "    n_cls_preds = len(unique_labels)\n",
    "    bbox_colors = random.sample(colors, n_cls_preds)\n",
    "    for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections_rescaled:\n",
    "\n",
    "        print(\"Label: %s, Conf: %.5f\" % (classes[int(cls_pred)], cls_conf.item()))\n",
    "\n",
    "        box_w = x2 - x1\n",
    "        box_h = y2 - y1\n",
    "\n",
    "        color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "        # Create a Rectangle patch\n",
    "        bbox = patches.Rectangle(\n",
    "            (x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\"\n",
    "        )\n",
    "        # Add the bbox to the plot\n",
    "        ax.add_patch(bbox)\n",
    "        # Add label\n",
    "        plt.text(\n",
    "            x1,\n",
    "            y1,\n",
    "            s=classes[int(cls_pred)],\n",
    "            color=\"white\",\n",
    "            verticalalignment=\"top\",\n",
    "            bbox={\"color\": color, \"pad\": 0},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-miniature",
   "metadata": {},
   "source": [
    "## Detekciós és ábrázolási ciklus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-mediterranean",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# fig = plt.figure(figsize=(12, 12))\n",
    "# ax = fig.add_subplot(111)\n",
    "# plt.ion()\n",
    "# # fig.show()\n",
    "# fig.canvas.draw()\n",
    "colors = [\"r\", \"b\"]\n",
    "\n",
    "img_dir = \"data/kitti/seq01\"\n",
    "img_size = 416  # Assuming an image size (modify as needed)\n",
    "\n",
    "# Set up the grid for the first 4 images\n",
    "rows = 2  # 2 rows\n",
    "cols = 2  # 2 columns\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 16))  # Adjust figure size as needed\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_name in enumerate(os.listdir(img_dir)):\n",
    "    if idx >= 4:  # Only process the first 4 images\n",
    "        break\n",
    "\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    img = np.array(Image.open(img_path).convert(\"RGB\"), dtype=np.uint8)\n",
    "    img_shape = img.shape[:2]\n",
    "\n",
    "    ax = axes[idx]\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Label Placeholder\n",
    "    boxes = np.zeros((1, 5))\n",
    "\n",
    "    img, _ = transforms.Compose([DEFAULT_TRANSFORMS, Resize(img_size)])((img, boxes))\n",
    "    img = Variable(img.type(torch.FloatTensor))\n",
    "    img.unsqueeze_(0)\n",
    "\n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(img)\n",
    "        detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "\n",
    "    # Draw bounding boxes and labels of detections\n",
    "    if detections[0] is not None:\n",
    "        detections = rescale_boxes(detections[0], img_size, img_shape)\n",
    "        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
    "            box_w = x2 - x1\n",
    "            box_h = y2 - y1\n",
    "            color = colors[int(cls_pred)]\n",
    "            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\")\n",
    "            ax.add_patch(bbox)\n",
    "            ax.text(x1, y1, s=classes[int(cls_pred)] + str(cls_conf), color=\"white\", verticalalignment=\"top\", bbox={\"color\": color, \"pad\": 0})\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Metrics\n",
    "\n",
    "## Precision / Recall\n",
    "![precision_recall](./doc/precision_recall.png)\n",
    "\n",
    "\n",
    "## F1\n",
    "![f1](./doc/f1.png)\n",
    "\n",
    "\n",
    "## Average Precision (AP)\n",
    "\n",
    "![ap](./doc/mean-average-precision-recall.png)\n",
    "\n",
    "\n",
    "## IoU\n",
    "![iou](./doc/iou-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-dominant",
   "metadata": {},
   "source": [
    "# Kiértékelés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-skill",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_dir = \"data/kitti/images/test\"\n",
    "label_dir = \"data/kitti/labels/test\"\n",
    "\n",
    "labels = []\n",
    "metrics = []\n",
    "\n",
    "for img_name in os.listdir(img_dir):\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    label_path = os.path.join(label_dir, img_name[:-4] + \".txt\")\n",
    "\n",
    "    # Load image\n",
    "    img = np.array(Image.open(img_path).convert(\"RGB\"), dtype=np.uint8)\n",
    "\n",
    "    # Load boxes\n",
    "    boxes = np.loadtxt(label_path).reshape(-1, 5)\n",
    "\n",
    "    img, boxes = transforms.Compose([DEFAULT_TRANSFORMS, Resize(img_size)])((img, boxes))\n",
    "\n",
    "    # Extract labels\n",
    "    labels += boxes[:, 1].tolist()\n",
    "    # Rescale boxes\n",
    "    boxes[:, 2:] = trans_xywh2xyxy(boxes[:, 2:])\n",
    "    boxes[:, 2:] *= img_size\n",
    "\n",
    "    # Configure input\n",
    "    img = Variable(img.type(torch.FloatTensor))\n",
    "    img.unsqueeze_(0)\n",
    "\n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(img)\n",
    "        detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "\n",
    "    # Gather batch statistics\n",
    "    metrics += get_batch_statistics(\n",
    "        detections, boxes, iou_threshold=iou_thres\n",
    "    )\n",
    "\n",
    "# Concatenate sample statistics\n",
    "true_positives, pred_scores, pred_labels = [\n",
    "    np.concatenate(x, 0) for x in list(zip(*metrics))\n",
    "]\n",
    "precision, recall, AP, f1, ap_class = ap_per_class(\n",
    "    true_positives, pred_scores, pred_labels, labels\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results:\\n\")\n",
    "print(\"Class-wise Metrics:\")\n",
    "for i, c in enumerate(ap_class):\n",
    "    print(f\"Class '{classes[c]}' (ID: {c}):\")\n",
    "    print(f\"    Average Precision: {AP[i]:.4f}\")\n",
    "    print(f\"    Precision: {precision[i]:.4f}\")\n",
    "    print(f\"    Recall: {recall[i]:.4f}\")\n",
    "    print(f\"    F1 Score: {f1[i]:.4f}\\n\")\n",
    "\n",
    "print(f\"Mean Average Precision (mAP): {AP.mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
